{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Injuries Information + Previous Season Statistics of the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:52:53.955937Z",
     "iopub.status.busy": "2025-04-08T19:52:53.955406Z",
     "iopub.status.idle": "2025-04-08T19:55:40.465172Z",
     "shell.execute_reply": "2025-04-08T19:55:40.463730Z",
     "shell.execute_reply.started": "2025-04-08T19:52:53.955891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import os\n",
    "import random\n",
    "\n",
    "#####################\n",
    "# Configurable Variables\n",
    "#####################\n",
    "API_KEY = \"xxx\"   # Replace with your API key\n",
    "SEASON = 2024\n",
    "PREV_SEASON = SEASON - 1\n",
    "\n",
    "SEASON_START = f\"{SEASON}-08-01\"   # e.g. \"2024-08-01\"\n",
    "FIXTURE_END = f\"{SEASON+1}-05-31\"  # e.g. \"2025-05-31\"\n",
    "\n",
    "NUM_PREV_FIXTURES = 5\n",
    "PLAYER_IDS = list(range(1500, 2000000))  # Large range for demonstration\n",
    "\n",
    "SAVE_EVERY_N_ROWS = 10\n",
    "TARGET_NEGATIVE_RECORDS = 5000\n",
    "CHECKPOINT_FILE = \"negative_scan_checkpoint.json\"\n",
    "\n",
    "MAX_WORKERS = 10\n",
    "MAX_RETRIES = 99999\n",
    "REQUEST_TIMEOUT = 10\n",
    "CACHE_DIR = \"api_cache\"\n",
    "\n",
    "# We pick how many random fixtures to pick per no-injury player\n",
    "NEG_SAMPLES_PER_PLAYER = 5\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "#####################\n",
    "# Cache & Connection Setup\n",
    "#####################\n",
    "cache = {}\n",
    "\n",
    "def get_cache_key(endpoint: str) -> str:\n",
    "    \"\"\"Return a filesystem-safe cache key from an endpoint URL.\"\"\"\n",
    "    return endpoint.replace(\"/\", \"_\").replace(\"?\", \"_\").replace(\"&\", \"_\").replace(\"=\", \"_\")\n",
    "\n",
    "def cache_api_response(endpoint: str, response: dict):\n",
    "    \"\"\"Store an API response in memory & on disk.\"\"\"\n",
    "    cache[endpoint] = response\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{get_cache_key(endpoint)}.json\")\n",
    "    try:\n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(response, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error caching to disk: {e}\")\n",
    "\n",
    "def get_cached_response(endpoint: str) -> dict:\n",
    "    \"\"\"Retrieve a cached API response from memory or disk, if available.\"\"\"\n",
    "    if endpoint in cache:\n",
    "        return cache[endpoint]\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{get_cache_key(endpoint)}.json\")\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, \"r\") as f:\n",
    "                response = json.load(f)\n",
    "                cache[endpoint] = response\n",
    "                return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading cache: {e}\")\n",
    "    return {}\n",
    "\n",
    "def create_connection():\n",
    "    \"\"\"Create an HTTPS connection to API-Football.\"\"\"\n",
    "    return http.client.HTTPSConnection(\"v3.football.api-sports.io\", timeout=REQUEST_TIMEOUT)\n",
    "\n",
    "def make_api_request(endpoint: str, headers: dict = None) -> dict:\n",
    "    \"\"\"\n",
    "    Make an API request with caching, handling rate limits via exponential backoff.\n",
    "    Returns a dict response (or empty dict on failure).\n",
    "    \"\"\"\n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            \"x-rapidapi-host\": \"v3.football.api-sports.io\",\n",
    "            \"x-rapidapi-key\": API_KEY\n",
    "        }\n",
    "    cached = get_cached_response(endpoint)\n",
    "    if cached:\n",
    "        return cached\n",
    "    \n",
    "    base_wait = 5\n",
    "    max_wait = 120\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            conn = create_connection()\n",
    "            conn.request(\"GET\", endpoint, headers=headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read()\n",
    "            response = json.loads(data.decode(\"utf-8\"))\n",
    "            \n",
    "            # If rate-limited or partial errors\n",
    "            is_rate_limited = (res.status == 429)\n",
    "            has_rate_error = \"rateLimit\" in response.get(\"errors\", {})\n",
    "            if is_rate_limited or has_rate_error:\n",
    "                wait_time = min(base_wait * (2**attempt), max_wait)\n",
    "                print(f\"Rate limited. Waiting {wait_time}s before retry {attempt+1} for {endpoint}\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            cache_api_response(endpoint, response)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt+1} for {endpoint}: {e}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                wait_time = min(base_wait * (2**attempt), max_wait)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Max retries reached for {endpoint}. Giving up.\")\n",
    "                return {}\n",
    "        finally:\n",
    "            if 'conn' in locals():\n",
    "                conn.close()\n",
    "    return {}\n",
    "\n",
    "#####################\n",
    "# Flatten Player Stats\n",
    "#####################\n",
    "def flatten_player_stats(player_response: dict) -> list:\n",
    "    \"\"\"Flatten /players response into a list of dictionaries (1+ per league/team statistic).\"\"\"\n",
    "    flattened_list = []\n",
    "    player_info = player_response.get(\"player\", {})\n",
    "    player_dict = {\n",
    "        \"player_id\": player_info.get(\"id\"),\n",
    "        \"player_name\": player_info.get(\"name\"),\n",
    "        \"player_firstname\": player_info.get(\"firstname\"),\n",
    "        \"player_lastname\": player_info.get(\"lastname\"),\n",
    "        \"player_age\": player_info.get(\"age\"),\n",
    "        \"player_birth_date\": player_info.get(\"birth\", {}).get(\"date\"),\n",
    "        \"player_birth_place\": player_info.get(\"birth\", {}).get(\"place\"),\n",
    "        \"player_birth_country\": player_info.get(\"birth\", {}).get(\"country\"),\n",
    "        \"player_nationality\": player_info.get(\"nationality\"),\n",
    "        \"player_height\": player_info.get(\"height\"),\n",
    "        \"player_weight\": player_info.get(\"weight\"),\n",
    "        \"player_injured\": player_info.get(\"injured\"),\n",
    "        \"player_photo\": player_info.get(\"photo\")\n",
    "    }\n",
    "    statistics = player_response.get(\"statistics\", [])\n",
    "    if not statistics:\n",
    "        flattened_list.append(player_dict)\n",
    "    else:\n",
    "        for stat in statistics:\n",
    "            record = player_dict.copy()\n",
    "            team = stat.get(\"team\", {})\n",
    "            record[\"team_id\"] = team.get(\"id\")\n",
    "            record[\"team_name\"] = team.get(\"name\")\n",
    "            record[\"team_logo\"] = team.get(\"logo\")\n",
    "            league = stat.get(\"league\", {})\n",
    "            record[\"league_id\"] = league.get(\"id\")\n",
    "            record[\"league_name\"] = league.get(\"name\")\n",
    "            record[\"league_country\"] = league.get(\"country\")\n",
    "            record[\"league_logo\"] = league.get(\"logo\")\n",
    "            record[\"league_flag\"] = league.get(\"flag\")\n",
    "            record[\"league_season\"] = league.get(\"season\")\n",
    "            games = stat.get(\"games\", {})\n",
    "            record[\"games_appearences\"] = games.get(\"appearences\")\n",
    "            record[\"games_lineups\"] = games.get(\"lineups\")\n",
    "            record[\"games_minutes\"] = games.get(\"minutes\")\n",
    "            record[\"games_number\"] = games.get(\"number\")\n",
    "            record[\"games_position\"] = games.get(\"position\")\n",
    "            record[\"games_rating\"] = games.get(\"rating\")\n",
    "            record[\"games_captain\"] = games.get(\"captain\")\n",
    "            substitutes = stat.get(\"substitutes\", {})\n",
    "            record[\"substitutes_in\"] = substitutes.get(\"in\")\n",
    "            record[\"substitutes_out\"] = substitutes.get(\"out\")\n",
    "            record[\"substitutes_bench\"] = substitutes.get(\"bench\")\n",
    "            shots = stat.get(\"shots\", {})\n",
    "            record[\"shots_total\"] = shots.get(\"total\")\n",
    "            record[\"shots_on\"] = shots.get(\"on\")\n",
    "            goals = stat.get(\"goals\", {})\n",
    "            record[\"goals_total\"] = goals.get(\"total\")\n",
    "            record[\"goals_conceded\"] = goals.get(\"conceded\")\n",
    "            record[\"goals_assists\"] = goals.get(\"assists\")\n",
    "            record[\"goals_saves\"] = goals.get(\"saves\")\n",
    "            passes = stat.get(\"passes\", {})\n",
    "            record[\"passes_total\"] = passes.get(\"total\")\n",
    "            record[\"passes_key\"] = passes.get(\"key\")\n",
    "            record[\"passes_accuracy\"] = passes.get(\"accuracy\")\n",
    "            tackles = stat.get(\"tackles\", {})\n",
    "            record[\"tackles_total\"] = tackles.get(\"total\")\n",
    "            record[\"tackles_blocks\"] = tackles.get(\"blocks\")\n",
    "            record[\"tackles_interceptions\"] = tackles.get(\"interceptions\")\n",
    "            duels = stat.get(\"duels\", {})\n",
    "            record[\"duels_total\"] = duels.get(\"total\")\n",
    "            record[\"duels_won\"] = duels.get(\"won\")\n",
    "            dribbles = stat.get(\"dribbles\", {})\n",
    "            record[\"dribbles_attempts\"] = dribbles.get(\"attempts\")\n",
    "            record[\"dribbles_success\"] = dribbles.get(\"success\")\n",
    "            record[\"dribbles_past\"] = dribbles.get(\"past\")\n",
    "            fouls = stat.get(\"fouls\", {})\n",
    "            record[\"fouls_drawn\"] = fouls.get(\"drawn\")\n",
    "            record[\"fouls_committed\"] = fouls.get(\"committed\")\n",
    "            cards = stat.get(\"cards\", {})\n",
    "            record[\"cards_yellow\"] = cards.get(\"yellow\")\n",
    "            record[\"cards_yellowred\"] = cards.get(\"yellowred\")\n",
    "            record[\"cards_red\"] = cards.get(\"red\")\n",
    "            penalty = stat.get(\"penalty\", {})\n",
    "            record[\"penalty_won\"] = penalty.get(\"won\")\n",
    "            record[\"penalty_commited\"] = penalty.get(\"commited\")\n",
    "            record[\"penalty_scored\"] = penalty.get(\"scored\")\n",
    "            record[\"penalty_missed\"] = penalty.get(\"missed\")\n",
    "            record[\"penalty_saved\"] = penalty.get(\"saved\")\n",
    "            flattened_list.append(record)\n",
    "    return flattened_list\n",
    "\n",
    "#######################\n",
    "# Negative Sample Logic\n",
    "#######################\n",
    "def gather_negative_matches_for_no_injury_player(player_id: int) -> list:\n",
    "    \"\"\"\n",
    "    For a player with zero injuries:\n",
    "      1) /players?id=xxx&season={SEASON} to find team_id (and league_id).\n",
    "      2) /fixtures?team=xxx to gather that team's fixtures this season.\n",
    "      3) Filter valid, completed fixtures (status FT, AET, PEN).\n",
    "      4) Randomly sample up to NEG_SAMPLES_PER_PLAYER.\n",
    "      5) Build negative sample dict (mirroring the positive sample structure).\n",
    "    \"\"\"\n",
    "    negative_samples = []\n",
    "    \n",
    "    # 1) retrieve player's aggregated stats for this season\n",
    "    endpoint_stats = f\"/players?id={player_id}&season={SEASON}\"\n",
    "    json_stats = make_api_request(endpoint_stats)\n",
    "    if not json_stats or not json_stats.get(\"response\"):\n",
    "        return negative_samples\n",
    "    \n",
    "    try:\n",
    "        player_resp = json_stats[\"response\"][0]\n",
    "        flist = flatten_player_stats(player_resp)\n",
    "        if not flist:\n",
    "            return negative_samples\n",
    "        team_id = flist[0].get(\"team_id\")\n",
    "        league_id = flist[0].get(\"league_id\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting team info for player {player_id}: {e}\")\n",
    "        return negative_samples\n",
    "    \n",
    "    # 2) gather that team's fixtures\n",
    "    endpoint_fixtures = f\"/fixtures?team={team_id}&season={SEASON}&from={SEASON_START}&to={FIXTURE_END}\"\n",
    "    json_fixtures = make_api_request(endpoint_fixtures)\n",
    "    if not json_fixtures or not json_fixtures.get(\"response\"):\n",
    "        return negative_samples\n",
    "    \n",
    "    fixtures = json_fixtures.get(\"response\", [])\n",
    "    \n",
    "    # 3) filter only valid, completed matches\n",
    "    valid_fixtures = []\n",
    "    for fix in fixtures:\n",
    "        fdata = fix.get(\"fixture\", {})\n",
    "        if not fdata or not fdata.get(\"id\") or not fdata.get(\"date\"):\n",
    "            continue\n",
    "        status_short = fdata.get(\"status\", {}).get(\"short\")\n",
    "        if status_short not in (\"FT\", \"AET\", \"PEN\"):\n",
    "            continue\n",
    "        \n",
    "        valid_fixtures.append(fix)\n",
    "    \n",
    "    if not valid_fixtures:\n",
    "        return negative_samples\n",
    "    \n",
    "    # 4) random sample\n",
    "    sampled_fixtures = random.sample(valid_fixtures, min(NEG_SAMPLES_PER_PLAYER, len(valid_fixtures)))\n",
    "    \n",
    "    # 5) build negative sample records\n",
    "    for fix in sampled_fixtures:\n",
    "        try:\n",
    "            fid = int(fix[\"fixture\"][\"id\"])\n",
    "        except:\n",
    "            fid = None\n",
    "        negative_samples.append({\n",
    "            \"injury_player_id\": player_id,\n",
    "            \"fixture_id\": fid,\n",
    "            \"injury_date\": fix[\"fixture\"][\"date\"],\n",
    "            \"injury_team_id\": team_id,\n",
    "            \"injury_league_id\": league_id,\n",
    "            \"injury_season\": SEASON,\n",
    "            \"reason\": \"no_injury_in_season\",\n",
    "            \"injuried\": 0\n",
    "        })\n",
    "    return negative_samples\n",
    "\n",
    "######################################\n",
    "# Opponent + Previous 5 Fixtures Logic\n",
    "######################################\n",
    "def get_opponent_team_id(fixture_id, injury_team_id):\n",
    "    \"\"\"Find the opponent team ID by calling /fixtures?fixture=xxx or stats fallback.\"\"\"\n",
    "    if not fixture_id:\n",
    "        return None\n",
    "    endpoint = f\"/fixtures?fixture={fixture_id}\"\n",
    "    resp = make_api_request(endpoint)\n",
    "    fixtures = resp.get(\"response\", []) if resp else []\n",
    "    \n",
    "    opp = None\n",
    "    if fixtures:\n",
    "        teams = fixtures[0].get(\"teams\", {})\n",
    "        home = teams.get(\"home\", {}).get(\"id\")\n",
    "        away = teams.get(\"away\", {}).get(\"id\")\n",
    "        try:\n",
    "            home = int(home) if home is not None else None\n",
    "            away = int(away) if away is not None else None\n",
    "        except:\n",
    "            home, away = None, None\n",
    "        if injury_team_id == home:\n",
    "            opp = away\n",
    "        elif injury_team_id == away:\n",
    "            opp = home\n",
    "    \n",
    "    if opp is None:\n",
    "        # fallback: /fixtures/statistics\n",
    "        endpoint2 = f\"/fixtures/statistics?fixture={fixture_id}\"\n",
    "        stats_resp = make_api_request(endpoint2)\n",
    "        responses = stats_resp.get(\"response\", []) if stats_resp else []\n",
    "        if len(responses) == 2:\n",
    "            try:\n",
    "                t1 = int(responses[0].get(\"team\", {}).get(\"id\"))\n",
    "                t2 = int(responses[1].get(\"team\", {}).get(\"id\"))\n",
    "            except:\n",
    "                t1, t2 = None, None\n",
    "            if injury_team_id == t1:\n",
    "                opp = t2\n",
    "            elif injury_team_id == t2:\n",
    "                opp = t1\n",
    "    return opp\n",
    "\n",
    "def get_previous_fixture_ids(team_id, season, current_fixture_date, current_fixture_id,\n",
    "                             k=NUM_PREV_FIXTURES, season_start=SEASON_START):\n",
    "    \"\"\"Get IDs of the previous k fixtures for a given team prior to current_fixture_date.\"\"\"\n",
    "    if not team_id or not current_fixture_date:\n",
    "        return [None]*k\n",
    "    date_only = current_fixture_date.split(\"T\")[0]\n",
    "    endpoint = f\"/fixtures?team={team_id}&season={season}&from={season_start}&to={date_only}\"\n",
    "    resp = make_api_request(endpoint)\n",
    "    fixtures = resp.get(\"response\", []) if resp else []\n",
    "    prev_fixtures = []\n",
    "    for fix in fixtures:\n",
    "        try:\n",
    "            fid = int(fix[\"fixture\"][\"id\"])\n",
    "        except:\n",
    "            fid = None\n",
    "        if fid and fid != current_fixture_id:\n",
    "            prev_fixtures.append((fix[\"fixture\"][\"date\"], fid))\n",
    "    if not prev_fixtures:\n",
    "        return [None]*k\n",
    "    # Sort descending\n",
    "    prev_fixtures.sort(key=lambda x: x[0], reverse=True)\n",
    "    prev_ids = [fid for _, fid in prev_fixtures[:k]]\n",
    "    while len(prev_ids) < k:\n",
    "        prev_ids.append(None)\n",
    "    return prev_ids\n",
    "\n",
    "def process_fixture_info_for_neg(row_data: dict) -> dict:\n",
    "    \"\"\"Replicate expansions for negative samples: opponent ID + 5 previous fixtures each side.\"\"\"\n",
    "    try:\n",
    "        fix_id = int(row_data[\"fixture_id\"]) if row_data[\"fixture_id\"] else None\n",
    "    except:\n",
    "        fix_id = None\n",
    "    try:\n",
    "        team_id = int(row_data[\"injury_team_id\"]) if row_data[\"injury_team_id\"] else None\n",
    "    except:\n",
    "        team_id = None\n",
    "    dt = row_data.get(\"injury_date\")\n",
    "    try:\n",
    "        seas = int(row_data[\"injury_season\"])\n",
    "    except:\n",
    "        seas = None\n",
    "    \n",
    "    opp_id = get_opponent_team_id(fix_id, team_id)\n",
    "    inj_team_fixtures = get_previous_fixture_ids(team_id, seas, dt, fix_id)\n",
    "    if opp_id:\n",
    "        opp_team_fixtures = get_previous_fixture_ids(opp_id, seas, dt, fix_id)\n",
    "    else:\n",
    "        opp_team_fixtures = [None]*NUM_PREV_FIXTURES\n",
    "    \n",
    "    result = {\"opponent_team_id\": opp_id}\n",
    "    for j in range(NUM_PREV_FIXTURES):\n",
    "        result[f\"prev_inj_team_fixture_{j+1}\"] = inj_team_fixtures[j]\n",
    "        result[f\"prev_opp_team_fixture_{j+1}\"] = opp_team_fixtures[j]\n",
    "    return result\n",
    "\n",
    "####################################\n",
    "# Batching / Negative Sample Logic\n",
    "####################################\n",
    "def process_player_batch(player_ids: list) -> tuple:\n",
    "    \"\"\"\n",
    "    For each player:\n",
    "      1) Check if zero injuries for current season\n",
    "      2) If zero, gather negative fixtures from that player's team\n",
    "      3) Also gather previous-season stats\n",
    "    \"\"\"\n",
    "    local_negative_list = []\n",
    "    local_flattened_stats_prev = {}\n",
    "    \n",
    "    def process_single_player(player_id: int):\n",
    "        # Check injuries\n",
    "        endpoint_inj = f\"/injuries?player={player_id}&season={SEASON}\"\n",
    "        resp_inj = make_api_request(endpoint_inj)\n",
    "        injuries = resp_inj.get(\"response\", []) if resp_inj else []\n",
    "        if injuries:\n",
    "            # This player has injuries => skip as negative\n",
    "            return {\"player_id\": player_id, \"neg_samples\": [], \"prev_stats\": {}, \"is_negative\": False}\n",
    "        \n",
    "        # Gather negative samples\n",
    "        neg_samples = gather_negative_matches_for_no_injury_player(player_id)\n",
    "        \n",
    "        # Gather previous season stats\n",
    "        endp_prev = f\"/players?id={player_id}&season={PREV_SEASON}\"\n",
    "        resp_prev = make_api_request(endp_prev)\n",
    "        prev_stats = {}\n",
    "        if resp_prev and resp_prev.get(\"response\"):\n",
    "            try:\n",
    "                p_resp = resp_prev[\"response\"][0]\n",
    "                flist = flatten_player_stats(p_resp)\n",
    "                if flist:\n",
    "                    prev_stats = {\"prev_\" + k: v for k, v in flist[0].items()}\n",
    "            except Exception as e:\n",
    "                print(f\"Error flattening stats for {player_id}: {e}\")\n",
    "        \n",
    "        return {\"player_id\": player_id, \"neg_samples\": neg_samples, \"prev_stats\": prev_stats, \"is_negative\": True}\n",
    "    \n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_map = {executor.submit(process_single_player, pid): pid for pid in player_ids}\n",
    "        for fut in concurrent.futures.as_completed(future_map):\n",
    "            pid = future_map[fut]\n",
    "            try:\n",
    "                r = fut.result()\n",
    "                results.append(r)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing player {pid}: {e}\")\n",
    "    \n",
    "    for r in results:\n",
    "        if r[\"is_negative\"]:\n",
    "            local_negative_list.extend(r[\"neg_samples\"])\n",
    "            if r[\"prev_stats\"]:\n",
    "                local_flattened_stats_prev[r[\"player_id\"]] = r[\"prev_stats\"]\n",
    "    return local_negative_list, local_flattened_stats_prev\n",
    "\n",
    "#####################\n",
    "# Main Execution\n",
    "#####################\n",
    "def main():\n",
    "    negative_list = []\n",
    "    flattened_stats_prev = {}\n",
    "    \n",
    "    start_idx = 0\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "                start_idx = checkpoint.get(\"last_processed_idx\", 0)\n",
    "                negative_list = checkpoint.get(\"negative_list\", [])\n",
    "                flattened_stats_prev = checkpoint.get(\"flattened_stats_prev\", {})\n",
    "                print(f\"Resuming from checkpoint at index {start_idx}, found {len(negative_list)} negative samples so far.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    \n",
    "    remaining_pids = PLAYER_IDS[start_idx:]\n",
    "    print(f\"Scanning {len(remaining_pids)} players from {remaining_pids[0]} to {remaining_pids[-1]}\")\n",
    "    \n",
    "    BATCH_SIZE = 100\n",
    "    neg_count = len(negative_list)\n",
    "    \n",
    "    for batch_start in tqdm(range(0, len(remaining_pids), BATCH_SIZE), desc=\"Processing player batches\"):\n",
    "        if neg_count >= TARGET_NEGATIVE_RECORDS:\n",
    "            print(f\"Reached {TARGET_NEGATIVE_RECORDS} negative samples. Stopping early.\")\n",
    "            break\n",
    "        \n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(remaining_pids))\n",
    "        batch_ids = remaining_pids[batch_start:batch_end]\n",
    "        \n",
    "        batch_negatives, batch_prev_stats = process_player_batch(batch_ids)\n",
    "        negative_list.extend(batch_negatives)\n",
    "        neg_count += len(batch_negatives)\n",
    "        flattened_stats_prev.update(batch_prev_stats)\n",
    "        \n",
    "        global_idx = start_idx + batch_end\n",
    "        # Save checkpoint\n",
    "        if batch_end % (BATCH_SIZE * 5) == 0 or neg_count >= TARGET_NEGATIVE_RECORDS:\n",
    "            checkpoint = {\n",
    "                \"last_processed_idx\": global_idx,\n",
    "                \"negative_list\": negative_list,\n",
    "                \"flattened_stats_prev\": flattened_stats_prev\n",
    "            }\n",
    "            with open(CHECKPOINT_FILE, 'w') as f:\n",
    "                json.dump(checkpoint, f)\n",
    "            print(f\"Checkpoint saved at {global_idx}. Found {len(negative_list)} negative samples so far.\")\n",
    "    \n",
    "    print(f\"\\nCollected {len(negative_list)} total negative samples.\")\n",
    "    if not negative_list:\n",
    "        print(\"No negative samples found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Build negative DataFrame\n",
    "    df_neg = pd.DataFrame(negative_list)\n",
    "    df_neg.to_csv(\"negative_players.csv\", index=False)\n",
    "    print(\"Initial negative samples DataFrame (no expansions yet):\")\n",
    "    print(df_neg.head())\n",
    "    \n",
    "    # Expand with fixture info in parallel\n",
    "    print(\"Expanding fixture info for negative samples (opponent + prev 5 fixtures)...\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_map = {executor.submit(process_fixture_info_for_neg, row): i\n",
    "                      for i, row in df_neg.iterrows()}\n",
    "        expansions = []\n",
    "        for fut in tqdm(concurrent.futures.as_completed(future_map),\n",
    "                        total=len(future_map),\n",
    "                        desc=\"Expanding\"):\n",
    "            idx = future_map[fut]\n",
    "            try:\n",
    "                ex_result = fut.result()\n",
    "                expansions.append((idx, ex_result))\n",
    "            except Exception as e:\n",
    "                print(f\"Error expanding fixture for row {idx}: {e}\")\n",
    "                expansions.append((idx, {}))\n",
    "            \n",
    "            # Periodic checkpoint\n",
    "            if len(expansions) % SAVE_EVERY_N_ROWS == 0:\n",
    "                df_neg.to_csv(\"checkpoint_negative_expansions.csv\", index=False)\n",
    "                print(f\"Expansion checkpoint at {len(expansions)} processed rows.\")\n",
    "    \n",
    "    # Sort expansions by row index\n",
    "    expansions.sort(key=lambda x: x[0])\n",
    "    # Prepare columns\n",
    "    opponent_team_ids = []\n",
    "    prev_inj_team_fixtures = {f\"prev_inj_team_fixture_{i+1}\": [] for i in range(NUM_PREV_FIXTURES)}\n",
    "    prev_opp_team_fixtures = {f\"prev_opp_team_fixture_{i+1}\": [] for i in range(NUM_PREV_FIXTURES)}\n",
    "    \n",
    "    for _, expansion in expansions:\n",
    "        opponent_team_ids.append(expansion.get(\"opponent_team_id\"))\n",
    "        for j in range(NUM_PREV_FIXTURES):\n",
    "            col_inj = f\"prev_inj_team_fixture_{j+1}\"\n",
    "            col_opp = f\"prev_opp_team_fixture_{j+1}\"\n",
    "            prev_inj_team_fixtures[col_inj].append(expansion.get(col_inj))\n",
    "            prev_opp_team_fixtures[col_opp].append(expansion.get(col_opp))\n",
    "    \n",
    "    df_neg[\"opponent_team_id\"] = opponent_team_ids\n",
    "    for c, arr in prev_inj_team_fixtures.items():\n",
    "        df_neg[c] = arr\n",
    "    for c, arr in prev_opp_team_fixtures.items():\n",
    "        df_neg[c] = arr\n",
    "    \n",
    "    # Add previous-season info from flattened_stats_prev\n",
    "    # (the dictionary with \"prev_player_id\", \"prev_team_id\", etc.)\n",
    "    prev_info_list = []\n",
    "    for pid in df_neg[\"injury_player_id\"]:\n",
    "        info = flattened_stats_prev.get(pid, {})\n",
    "        prev_info_list.append(info)\n",
    "    if prev_info_list:\n",
    "        prev_df = pd.DataFrame(prev_info_list)\n",
    "        df_neg = pd.concat([df_neg.reset_index(drop=True), prev_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Convert fixture IDs to object for consistency\n",
    "    fixture_cols = [\"fixture_id\"] + [\n",
    "        f\"prev_inj_team_fixture_{i+1}\" for i in range(NUM_PREV_FIXTURES)\n",
    "    ] + [\n",
    "        f\"prev_opp_team_fixture_{i+1}\" for i in range(NUM_PREV_FIXTURES)\n",
    "    ]\n",
    "    for col in fixture_cols:\n",
    "        df_neg[col] = df_neg[col].astype(\"object\")\n",
    "    \n",
    "    # Final output\n",
    "    df_neg.to_csv(\"final_negative_df.csv\", index=False)\n",
    "    print(\"\\nFinal Negative Samples DataFrame with expansions & previous-season stats:\")\n",
    "    print(df_neg.head(10))\n",
    "    \n",
    "    print(\"Done! Negative samples now include expansions + previous-season data, just like positives.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:57:36.585243Z",
     "iopub.status.busy": "2025-04-08T19:57:36.584858Z",
     "iopub.status.idle": "2025-04-08T19:57:36.610999Z",
     "shell.execute_reply": "2025-04-08T19:57:36.609344Z",
     "shell.execute_reply.started": "2025-04-08T19:57:36.585216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"final_negative_df.csv\").dropna(subset=['prev_inj_team_fixture_1',\n",
    "       'prev_inj_team_fixture_2', 'prev_inj_team_fixture_3',\n",
    "       'prev_inj_team_fixture_4', 'prev_inj_team_fixture_5', 'prev_opp_team_fixture_1', 'prev_opp_team_fixture_2',\n",
    "       'prev_opp_team_fixture_3', 'prev_opp_team_fixture_4',\n",
    "       'prev_opp_team_fixture_5']).reset_index(drop=True).to_csv(\"final_negative_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:57:38.194798Z",
     "iopub.status.busy": "2025-04-08T19:57:38.194330Z",
     "iopub.status.idle": "2025-04-08T19:57:38.209031Z",
     "shell.execute_reply": "2025-04-08T19:57:38.207713Z",
     "shell.execute_reply.started": "2025-04-08T19:57:38.194763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"final_negative_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:57:39.662539Z",
     "iopub.status.busy": "2025-04-08T19:57:39.661958Z",
     "iopub.status.idle": "2025-04-08T19:57:39.676995Z",
     "shell.execute_reply": "2025-04-08T19:57:39.675741Z",
     "shell.execute_reply.started": "2025-04-08T19:57:39.662487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[['injury_player_id', 'fixture_id','injury_team_id',\n",
    "       'injury_league_id', 'injury_season','opponent_team_id', 'prev_inj_team_fixture_1',\n",
    "       'prev_inj_team_fixture_2', 'prev_inj_team_fixture_3',\n",
    "       'prev_inj_team_fixture_4', 'prev_inj_team_fixture_5',\n",
    "       'prev_opp_team_fixture_1', 'prev_opp_team_fixture_2',\n",
    "       'prev_opp_team_fixture_3', 'prev_opp_team_fixture_4',\n",
    "       'prev_opp_team_fixture_5']] = df[['injury_player_id', 'fixture_id','injury_team_id',\n",
    "       'injury_league_id', 'injury_season','opponent_team_id', 'prev_inj_team_fixture_1',\n",
    "       'prev_inj_team_fixture_2', 'prev_inj_team_fixture_3',\n",
    "       'prev_inj_team_fixture_4', 'prev_inj_team_fixture_5',\n",
    "       'prev_opp_team_fixture_1', 'prev_opp_team_fixture_2',\n",
    "       'prev_opp_team_fixture_3', 'prev_opp_team_fixture_4',\n",
    "       'prev_opp_team_fixture_5']].fillna(-1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Previous 5 Matches Player, Team and Opponent Raw Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:57:43.474513Z",
     "iopub.status.busy": "2025-04-08T19:57:43.474164Z",
     "iopub.status.idle": "2025-04-08T19:58:24.458670Z",
     "shell.execute_reply": "2025-04-08T19:58:24.457252Z",
     "shell.execute_reply.started": "2025-04-08T19:57:43.474487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from functools import lru_cache\n",
    "\n",
    "#####################\n",
    "# 0. CONFIG / SETUP\n",
    "#####################\n",
    "API_KEY = \"xxx\"   # Replace with your API key\n",
    "TIME_BETWEEN_CALLS = 0.05  # Small delay between parallel batches\n",
    "MAX_RETRIES = 9999999            # How many times to retry if a call fails\n",
    "RETRY_WAIT_SECONDS = 2.0   # Wait this long before each retry\n",
    "MAX_WORKERS = 10           # Number of parallel workers (adjust based on API limits)\n",
    "\n",
    "# Create connection pool\n",
    "def create_connection():\n",
    "    conn = http.client.HTTPSConnection(\"v3.football.api-sports.io\")\n",
    "    return conn\n",
    "\n",
    "# Cache for API responses to avoid duplicate calls\n",
    "CACHE = {}\n",
    "\n",
    "#####################\n",
    "# 1. HELPER FUNCTIONS\n",
    "#####################\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def do_api_call(endpoint):\n",
    "    \"\"\"\n",
    "    Makes an HTTPS request to `endpoint`, parses JSON, and returns it.\n",
    "    Uses LRU cache to avoid duplicate calls and implements retry logic.\n",
    "    \"\"\"\n",
    "    # Check if in cache\n",
    "    if endpoint in CACHE:\n",
    "        return CACHE[endpoint]\n",
    "    \n",
    "    conn = create_connection()\n",
    "    headers = {\n",
    "        \"x-rapidapi-host\": \"v3.football.api-sports.io\",\n",
    "        \"x-rapidapi-key\": API_KEY\n",
    "    }\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            conn.request(\"GET\", endpoint, headers=headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read()\n",
    "            \n",
    "            # Check if rate limited (status code 429)\n",
    "            if res.status == 429:\n",
    "                wait_time = RETRY_WAIT_SECONDS * (2 ** attempt)  # Exponential backoff\n",
    "                print(f\"Rate limited. Waiting {wait_time}s before retry {attempt+1}\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "                \n",
    "            # Attempt to parse JSON\n",
    "            json_data = json.loads(data.decode(\"utf-8\"))\n",
    "            \n",
    "            # Cache the result\n",
    "            CACHE[endpoint] = json_data\n",
    "            return json_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt+1} for endpoint {endpoint}: {e}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                # Exponential backoff\n",
    "                wait_time = RETRY_WAIT_SECONDS * (2 ** attempt)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Max retries reached for {endpoint}. Giving up.\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_player_stats_for_fixture(fixture_id, player_id):\n",
    "    \"\"\"Get stats for a specific player in a fixture\"\"\"\n",
    "    if fixture_id is None:\n",
    "        return None\n",
    "\n",
    "    endpoint = f\"/fixtures/players?fixture={fixture_id}\"\n",
    "    json_data = do_api_call(endpoint)\n",
    "    if not json_data:\n",
    "        return None\n",
    "    \n",
    "    # Go through the response to find player stats\n",
    "    for team_info in json_data.get(\"response\", []):\n",
    "        players = team_info.get(\"players\", [])\n",
    "        for p in players:\n",
    "            if p[\"player\"][\"id\"] == player_id:\n",
    "                return p[\"statistics\"][0] if p[\"statistics\"] else None\n",
    "    return None\n",
    "\n",
    "def get_team_stats_for_fixture(fixture_id, team_id):\n",
    "    \"\"\"Get team statistics for a specific fixture\"\"\"\n",
    "    if fixture_id is None or team_id is None:\n",
    "        return None\n",
    "\n",
    "    endpoint = f\"/fixtures/statistics?fixture={fixture_id}&team={team_id}\"\n",
    "    json_data = do_api_call(endpoint)\n",
    "    if not json_data or not json_data.get(\"response\"):\n",
    "        return None\n",
    "    \n",
    "    team_stats_obj = json_data[\"response\"][0]\n",
    "    raw_stats = team_stats_obj.get(\"statistics\", [])\n",
    "    stats_dict = {}\n",
    "    for stat_item in raw_stats:\n",
    "        key = stat_item[\"type\"]\n",
    "        val = stat_item[\"value\"]\n",
    "        key_norm = key.lower().replace(\" \", \"_\")\n",
    "        stats_dict[key_norm] = val\n",
    "    return stats_dict\n",
    "\n",
    "#####################\n",
    "# 2. PARALLEL PROCESSING\n",
    "#####################\n",
    "\n",
    "def process_row(row):\n",
    "    \"\"\"Process a single row in parallel\"\"\"\n",
    "    player_id = row[\"injury_player_id\"]\n",
    "    NUM_PREV_FIXTURES = 5\n",
    "    \n",
    "    # Prepare lists to hold results\n",
    "    player_team_stats_list = []\n",
    "    team_agg_stats_list = []\n",
    "    opp_team_stats_list = []\n",
    "    \n",
    "    # Collect all tasks we need to run\n",
    "    tasks = []\n",
    "    \n",
    "    # Player's own team last 5 fixtures\n",
    "    for i in range(1, NUM_PREV_FIXTURES+1):\n",
    "        fix_id_col = f\"prev_inj_team_fixture_{i}\"\n",
    "        fix_id = row[fix_id_col]\n",
    "        if fix_id:\n",
    "            # Add player stats task\n",
    "            tasks.append({\n",
    "                'type': 'player',\n",
    "                'fixture_id': fix_id,\n",
    "                'player_id': player_id,\n",
    "                'index': i-1\n",
    "            })\n",
    "            \n",
    "            # Add team stats task\n",
    "            tasks.append({\n",
    "                'type': 'team',\n",
    "                'fixture_id': fix_id,\n",
    "                'team_id': row[\"injury_team_id\"],\n",
    "                'index': i-1\n",
    "            })\n",
    "    \n",
    "    # Opponent's last 5 fixtures\n",
    "    for i in range(1, NUM_PREV_FIXTURES+1):\n",
    "        fix_id_col = f\"prev_opp_team_fixture_{i}\"\n",
    "        fix_id = row[fix_id_col]\n",
    "        if fix_id:\n",
    "            # Add opponent stats task\n",
    "            tasks.append({\n",
    "                'type': 'opponent',\n",
    "                'fixture_id': fix_id,\n",
    "                'team_id': row[\"opponent_team_id\"],\n",
    "                'index': i-1\n",
    "            })\n",
    "    \n",
    "    # Initialize result lists with None placeholders\n",
    "    player_team_stats_list = [None] * NUM_PREV_FIXTURES\n",
    "    team_agg_stats_list = [None] * NUM_PREV_FIXTURES\n",
    "    opp_team_stats_list = [None] * NUM_PREV_FIXTURES\n",
    "    \n",
    "    # Execute all tasks (in a single thread since we're already in a worker)\n",
    "    for task in tasks:\n",
    "        if task['type'] == 'player':\n",
    "            result = get_player_stats_for_fixture(task['fixture_id'], task['player_id'])\n",
    "            player_team_stats_list[task['index']] = result\n",
    "        elif task['type'] == 'team':\n",
    "            result = get_team_stats_for_fixture(task['fixture_id'], task['team_id'])\n",
    "            team_agg_stats_list[task['index']] = result\n",
    "        elif task['type'] == 'opponent':\n",
    "            result = get_team_stats_for_fixture(task['fixture_id'], task['team_id'])\n",
    "            opp_team_stats_list[task['index']] = result\n",
    "    \n",
    "    return {\n",
    "        'player_team_fixture_stats': player_team_stats_list,\n",
    "        'team_fixture_agg_stats': team_agg_stats_list,\n",
    "        'opp_fixture_agg_stats': opp_team_stats_list\n",
    "    }\n",
    "\n",
    "def batch_process_rows(df):\n",
    "    \"\"\"Process all rows in parallel\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Create a mapping of future to row index\n",
    "        future_to_idx = {\n",
    "            executor.submit(process_row, row): idx \n",
    "            for idx, row in df.iterrows()\n",
    "        }\n",
    "        \n",
    "        # Process as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_idx), \n",
    "                          total=len(future_to_idx),\n",
    "                          desc=\"Processing rows\"):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append((idx, result))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {e}\")\n",
    "                all_results.append((idx, None))\n",
    "    \n",
    "    # Sort results by original index\n",
    "    all_results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Extract results in correct order\n",
    "    player_team_fixture_stats = []\n",
    "    team_fixture_agg_stats = []\n",
    "    opp_fixture_agg_stats = []\n",
    "    \n",
    "    for _, result in all_results:\n",
    "        if result:\n",
    "            player_team_fixture_stats.append(result['player_team_fixture_stats'])\n",
    "            team_fixture_agg_stats.append(result['team_fixture_agg_stats'])\n",
    "            opp_fixture_agg_stats.append(result['opp_fixture_agg_stats'])\n",
    "        else:\n",
    "            # Handle error case\n",
    "            player_team_fixture_stats.append([None] * 5)\n",
    "            team_fixture_agg_stats.append([None] * 5)\n",
    "            opp_fixture_agg_stats.append([None] * 5)\n",
    "    \n",
    "    return player_team_fixture_stats, team_fixture_agg_stats, opp_fixture_agg_stats\n",
    "\n",
    "#####################\n",
    "# 3. MAIN EXECUTION\n",
    "#####################\n",
    "\n",
    "def main():\n",
    "    print(\"Existing columns in df:\", df.columns)\n",
    "    \n",
    "    # Process all rows in parallel\n",
    "    print(\"Starting parallel processing of API calls...\")\n",
    "    player_team_fixture_stats, team_fixture_agg_stats, opp_fixture_agg_stats = batch_process_rows(df)\n",
    "    \n",
    "    # Now store them in new columns\n",
    "    df[\"raw_player_team_fixture_stats\"] = player_team_fixture_stats\n",
    "    df[\"raw_team_fixture_agg_stats\"] = team_fixture_agg_stats\n",
    "    df[\"raw_opp_fixture_agg_stats\"] = opp_fixture_agg_stats\n",
    "    \n",
    "    print(\"\\nStage 1 complete! df now has raw data columns:\")\n",
    "    print(df[[\"raw_player_team_fixture_stats\", \n",
    "              \"raw_team_fixture_agg_stats\",\n",
    "              \"raw_opp_fixture_agg_stats\"]].head())\n",
    "    \n",
    "    # Print cache stats\n",
    "    print(f\"Cache hits: {do_api_call.cache_info().hits}\")\n",
    "    print(f\"Cache misses: {do_api_call.cache_info().misses}\")\n",
    "    print(f\"Cache size: {len(CACHE)} entries\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:58:28.704827Z",
     "iopub.status.busy": "2025-04-08T19:58:28.704325Z",
     "iopub.status.idle": "2025-04-08T19:58:28.735151Z",
     "shell.execute_reply": "2025-04-08T19:58:28.733823Z",
     "shell.execute_reply.started": "2025-04-08T19:58:28.704798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"stage1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate the Raw Features and Derive More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:58:29.692983Z",
     "iopub.status.busy": "2025-04-08T19:58:29.692577Z",
     "iopub.status.idle": "2025-04-08T19:58:30.345785Z",
     "shell.execute_reply": "2025-04-08T19:58:30.344423Z",
     "shell.execute_reply.started": "2025-04-08T19:58:29.692953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# STAGE 2: FEATURE ENGINEERING\n",
    "#########################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "########################################################\n",
    "# 0) Rename Columns if Needed\n",
    "########################################################\n",
    "# Suppose your main DataFrame is called df and currently has columns like:\n",
    "#  [\"injury_player_id\", \"injury_date\", \"fixture_id\", \"injury_team_id\", ...]\n",
    "# But we want to rename them to simpler names: \"player_id\", \"date\", \"team_id\", ...\n",
    "# so the code below is consistent.\n",
    "\n",
    "df = df.rename(columns={\n",
    "    \"injury_player_id\": \"player_id\",\n",
    "    \"injury_date\": \"date\",\n",
    "    \"injury_team_id\": \"team_id\",\n",
    "    \"injury_league_id\": \"league_id\",\n",
    "})\n",
    "\n",
    "# The same if your injuries DataFrame has those columns:\n",
    "df_injuries = df[[\"player_id\",\"date\",\"reason\",\"injuried\"]].copy()\n",
    "\n",
    "########################################################\n",
    "# 1) Helpers for Parsing and Safe Operations\n",
    "########################################################\n",
    "\n",
    "def safe_float(x):\n",
    "    \"\"\"Convert x to float or return np.nan if not possible.\"\"\"\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def safe_int(x):\n",
    "    \"\"\"Convert x to int or return np.nan if not possible.\"\"\"\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "########################################################\n",
    "# 2) Aggregators for Player Stats\n",
    "########################################################\n",
    "\n",
    "def aggregate_player_stats(stats_list):\n",
    "    \"\"\"\n",
    "    stats_list is a list of up to 5 dicts, each representing\n",
    "    the player's stats for one fixture (from 'raw_player_team_fixture_stats').\n",
    "    We'll sum or average relevant fields across these fixtures.\n",
    "    \"\"\"\n",
    "    minutes_list, rating_list = [], []\n",
    "\n",
    "    shots_total_list, shots_on_list = [], []\n",
    "    goals_total_list, assists_list = [], []\n",
    "    fouls_committed_list, fouls_drawn_list = [], []\n",
    "    yellow_cards_list, red_cards_list = [], []\n",
    "    duels_total_list, duels_won_list = [], []\n",
    "    \n",
    "    passes_total_list, passes_key_list, passes_accuracy_list = [], [], []\n",
    "    tackles_total_list, tackles_blocks_list, tackles_interceptions_list = [], [], []\n",
    "    \n",
    "    for st in stats_list:\n",
    "        if not st:\n",
    "            # No stats => fill with NaN placeholders\n",
    "            minutes_list.append(np.nan)\n",
    "            rating_list.append(np.nan)\n",
    "\n",
    "            shots_total_list.append(np.nan)\n",
    "            shots_on_list.append(np.nan)\n",
    "            goals_total_list.append(np.nan)\n",
    "            assists_list.append(np.nan)\n",
    "\n",
    "            fouls_committed_list.append(np.nan)\n",
    "            fouls_drawn_list.append(np.nan)\n",
    "            yellow_cards_list.append(np.nan)\n",
    "            red_cards_list.append(np.nan)\n",
    "\n",
    "            duels_total_list.append(np.nan)\n",
    "            duels_won_list.append(np.nan)\n",
    "\n",
    "            passes_total_list.append(np.nan)\n",
    "            passes_key_list.append(np.nan)\n",
    "            passes_accuracy_list.append(np.nan)\n",
    "\n",
    "            tackles_total_list.append(np.nan)\n",
    "            tackles_blocks_list.append(np.nan)\n",
    "            tackles_interceptions_list.append(np.nan)\n",
    "        else:\n",
    "            g = st.get(\"games\", {})\n",
    "            minutes_list.append(safe_float(g.get(\"minutes\", np.nan)))\n",
    "            rating_list.append(safe_float(g.get(\"rating\", np.nan)))  # often a string, e.g. \"7.5\"\n",
    "\n",
    "            s = st.get(\"shots\", {})\n",
    "            shots_total_list.append(safe_float(s.get(\"total\", np.nan)))\n",
    "            shots_on_list.append(safe_float(s.get(\"on\", np.nan)))\n",
    "\n",
    "            gl = st.get(\"goals\", {})\n",
    "            goals_total_list.append(safe_float(gl.get(\"total\", np.nan)))\n",
    "            assists_list.append(safe_float(gl.get(\"assists\", np.nan)))\n",
    "\n",
    "            f = st.get(\"fouls\", {})\n",
    "            fouls_committed_list.append(safe_float(f.get(\"committed\", np.nan)))\n",
    "            fouls_drawn_list.append(safe_float(f.get(\"drawn\", np.nan)))\n",
    "\n",
    "            c = st.get(\"cards\", {})\n",
    "            yellow_cards_list.append(safe_float(c.get(\"yellow\", np.nan)))\n",
    "            red_cards_list.append(safe_float(c.get(\"red\", np.nan)))\n",
    "\n",
    "            d = st.get(\"duels\", {})\n",
    "            duels_total_list.append(safe_float(d.get(\"total\", np.nan)))\n",
    "            duels_won_list.append(safe_float(d.get(\"won\", np.nan)))\n",
    "\n",
    "            p = st.get(\"passes\", {})\n",
    "            passes_total_list.append(safe_float(p.get(\"total\", np.nan)))\n",
    "            passes_key_list.append(safe_float(p.get(\"key\", np.nan)))\n",
    "            passes_accuracy_list.append(safe_float(p.get(\"accuracy\", np.nan)))\n",
    "\n",
    "            t = st.get(\"tackles\", {})\n",
    "            tackles_total_list.append(safe_float(t.get(\"total\", np.nan)))\n",
    "            tackles_blocks_list.append(safe_float(t.get(\"blocks\", np.nan)))\n",
    "            tackles_interceptions_list.append(safe_float(t.get(\"interceptions\", np.nan)))\n",
    "\n",
    "    # Summations\n",
    "    shots_total_5 = np.nansum(shots_total_list)\n",
    "    duels_total_5 = np.nansum(duels_total_list)\n",
    "\n",
    "    # Ratio example: duels won\n",
    "    duels_win_ratio_5 = np.nan\n",
    "    if duels_total_5 > 0:\n",
    "        duels_win_ratio_5 = np.nansum(duels_won_list) / duels_total_5\n",
    "\n",
    "    # Pass accuracy example\n",
    "    pass_acc_mean = np.nanmean(passes_accuracy_list)  # average across matches\n",
    "\n",
    "    features = {\n",
    "        # Averages\n",
    "        \"player_minutes_avg_5\":           np.nanmean(minutes_list),\n",
    "        \"player_rating_avg_5\":            np.nanmean(rating_list),\n",
    "\n",
    "        # Sums\n",
    "        \"player_shots_total_5\":           shots_total_5,\n",
    "        \"player_shots_on_5\":             np.nansum(shots_on_list),\n",
    "        \"player_goals_5\":                 np.nansum(goals_total_list),\n",
    "        \"player_assists_5\":               np.nansum(assists_list),\n",
    "        \"player_fouls_committed_5\":       np.nansum(fouls_committed_list),\n",
    "        \"player_fouls_drawn_5\":           np.nansum(fouls_drawn_list),\n",
    "        \"player_yellow_cards_5\":          np.nansum(yellow_cards_list),\n",
    "        \"player_red_cards_5\":             np.nansum(red_cards_list),\n",
    "        \"player_duels_total_5\":           duels_total_5,\n",
    "        \"player_duels_won_5\":             np.nansum(duels_won_list),\n",
    "        \"player_passes_total_5\":          np.nansum(passes_total_list),\n",
    "        \"player_passes_key_5\":            np.nansum(passes_key_list),\n",
    "        \"player_tackles_total_5\":         np.nansum(tackles_total_list),\n",
    "        \"player_tackles_blocks_5\":        np.nansum(tackles_blocks_list),\n",
    "        \"player_tackles_interceptions_5\": np.nansum(tackles_interceptions_list),\n",
    "\n",
    "        # Ratios\n",
    "        \"player_duels_win_ratio_5\":       duels_win_ratio_5,\n",
    "        \"player_pass_acc_mean_5\":         pass_acc_mean,\n",
    "    }\n",
    "\n",
    "    return features\n",
    "\n",
    "########################################################\n",
    "# 3) Aggregators for Team & Opponent Stats\n",
    "########################################################\n",
    "\n",
    "def aggregate_team_stats(stats_list):\n",
    "    sog_list, sof_list, total_shots_list = [], [], []\n",
    "    fouls_list, corners_list, offsides_list = [], [], []\n",
    "    ball_poss_list = []\n",
    "    yellow_cards_list, red_cards_list = [], []\n",
    "    passes_list, passes_accurate_list = [], []\n",
    "\n",
    "    for st in stats_list:\n",
    "        if not st:\n",
    "            sog_list.append(np.nan)\n",
    "            sof_list.append(np.nan)\n",
    "            total_shots_list.append(np.nan)\n",
    "            fouls_list.append(np.nan)\n",
    "            corners_list.append(np.nan)\n",
    "            offsides_list.append(np.nan)\n",
    "            ball_poss_list.append(np.nan)\n",
    "            yellow_cards_list.append(np.nan)\n",
    "            red_cards_list.append(np.nan)\n",
    "            passes_list.append(np.nan)\n",
    "            passes_accurate_list.append(np.nan)\n",
    "        else:\n",
    "            sog_list.append(safe_float(st.get(\"shots_on_goal\", np.nan)))\n",
    "            sof_list.append(safe_float(st.get(\"shots_off_goal\", np.nan)))\n",
    "            total_shots_list.append(safe_float(st.get(\"total_shots\", np.nan)))\n",
    "            fouls_list.append(safe_float(st.get(\"fouls\", np.nan)))\n",
    "            corners_list.append(safe_float(st.get(\"corner_kicks\", np.nan)))\n",
    "            offsides_list.append(safe_float(st.get(\"offsides\", np.nan)))\n",
    "\n",
    "            poss_str = st.get(\"ball_possession\", None)\n",
    "            if poss_str and isinstance(poss_str, str) and poss_str.endswith(\"%\"):\n",
    "                val = poss_str.replace(\"%\", \"\")\n",
    "                ball_poss_list.append(safe_float(val))\n",
    "            else:\n",
    "                ball_poss_list.append(np.nan)\n",
    "\n",
    "            yellow_cards_list.append(safe_float(st.get(\"yellow_cards\", np.nan)))\n",
    "            red_cards_list.append(safe_float(st.get(\"red_cards\", np.nan)))\n",
    "\n",
    "            passes_list.append(safe_float(st.get(\"total_passes\", np.nan)))\n",
    "            passes_accurate_list.append(safe_float(st.get(\"passes_accurate\", np.nan)))\n",
    "\n",
    "    sog_sum = np.nansum(sog_list)\n",
    "    passes_sum = np.nansum(passes_list)\n",
    "    passes_acc_sum = np.nansum(passes_accurate_list)\n",
    "\n",
    "    pass_accuracy_5 = np.nan\n",
    "    if passes_sum > 0:\n",
    "        pass_accuracy_5 = passes_acc_sum / passes_sum\n",
    "\n",
    "    features = {\n",
    "        \"team_shots_on_goal_5\":    sog_sum,\n",
    "        \"team_shots_off_goal_5\":   np.nansum(sof_list),\n",
    "        \"team_total_shots_5\":      np.nansum(total_shots_list),\n",
    "        \"team_fouls_5\":            np.nansum(fouls_list),\n",
    "        \"team_corners_5\":          np.nansum(corners_list),\n",
    "        \"team_offsides_5\":         np.nansum(offsides_list),\n",
    "        \"team_ball_poss_avg_5\":    np.nanmean(ball_poss_list),\n",
    "        \"team_yellow_cards_5\":     np.nansum(yellow_cards_list),\n",
    "        \"team_red_cards_5\":        np.nansum(red_cards_list),\n",
    "        \"team_passes_5\":           passes_sum,\n",
    "        \"team_passes_acc_5\":       passes_acc_sum,\n",
    "        \"team_pass_acc_ratio_5\":   pass_accuracy_5,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def aggregate_opponent_stats(stats_list):\n",
    "    sog_list, sof_list, total_shots_list = [], [], []\n",
    "    fouls_list, corners_list, offsides_list = [], [], []\n",
    "    ball_poss_list = []\n",
    "    yellow_cards_list, red_cards_list = [], []\n",
    "    passes_list, passes_accurate_list = [], []\n",
    "\n",
    "    for st in stats_list:\n",
    "        if not st:\n",
    "            sog_list.append(np.nan)\n",
    "            sof_list.append(np.nan)\n",
    "            total_shots_list.append(np.nan)\n",
    "            fouls_list.append(np.nan)\n",
    "            corners_list.append(np.nan)\n",
    "            offsides_list.append(np.nan)\n",
    "            ball_poss_list.append(np.nan)\n",
    "            yellow_cards_list.append(np.nan)\n",
    "            red_cards_list.append(np.nan)\n",
    "            passes_list.append(np.nan)\n",
    "            passes_accurate_list.append(np.nan)\n",
    "        else:\n",
    "            sog_list.append(safe_float(st.get(\"shots_on_goal\", np.nan)))\n",
    "            sof_list.append(safe_float(st.get(\"shots_off_goal\", np.nan)))\n",
    "            total_shots_list.append(safe_float(st.get(\"total_shots\", np.nan)))\n",
    "            fouls_list.append(safe_float(st.get(\"fouls\", np.nan)))\n",
    "            corners_list.append(safe_float(st.get(\"corner_kicks\", np.nan)))\n",
    "            offsides_list.append(safe_float(st.get(\"offsides\", np.nan)))\n",
    "\n",
    "            poss_str = st.get(\"ball_possession\", None)\n",
    "            if poss_str and isinstance(poss_str, str) and poss_str.endswith(\"%\"):\n",
    "                val = poss_str.replace(\"%\", \"\")\n",
    "                ball_poss_list.append(safe_float(val))\n",
    "            else:\n",
    "                ball_poss_list.append(np.nan)\n",
    "\n",
    "            yellow_cards_list.append(safe_float(st.get(\"yellow_cards\", np.nan)))\n",
    "            red_cards_list.append(safe_float(st.get(\"red_cards\", np.nan)))\n",
    "\n",
    "            passes_list.append(safe_float(st.get(\"total_passes\", np.nan)))\n",
    "            passes_accurate_list.append(safe_float(st.get(\"passes_accurate\", np.nan)))\n",
    "\n",
    "    sog_sum = np.nansum(sog_list)\n",
    "    passes_sum = np.nansum(passes_list)\n",
    "    passes_acc_sum = np.nansum(passes_accurate_list)\n",
    "\n",
    "    pass_accuracy_5 = np.nan\n",
    "    if passes_sum > 0:\n",
    "        pass_accuracy_5 = passes_acc_sum / passes_sum\n",
    "\n",
    "    features = {\n",
    "        \"opp_shots_on_goal_5\":   sog_sum,\n",
    "        \"opp_shots_off_goal_5\":  np.nansum(sof_list),\n",
    "        \"opp_total_shots_5\":     np.nansum(total_shots_list),\n",
    "        \"opp_fouls_5\":           np.nansum(fouls_list),\n",
    "        \"opp_corners_5\":         np.nansum(corners_list),\n",
    "        \"opp_offsides_5\":        np.nansum(offsides_list),\n",
    "        \"opp_ball_poss_avg_5\":   np.nanmean(ball_poss_list),\n",
    "        \"opp_yellow_cards_5\":    np.nansum(yellow_cards_list),\n",
    "        \"opp_red_cards_5\":       np.nansum(red_cards_list),\n",
    "        \"opp_passes_5\":          passes_sum,\n",
    "        \"opp_passes_acc_5\":      passes_acc_sum,\n",
    "        \"opp_pass_acc_ratio_5\":  pass_accuracy_5,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "########################################################\n",
    "# 4) Days Since Last Injury (Optional)\n",
    "########################################################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_days_since_last_injury(df_main, df_injuries):\n",
    "    \"\"\"\n",
    "    Compute days since the last injury for each player in df_main.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_main : pandas DataFrame\n",
    "        Main DataFrame containing player_id and date columns\n",
    "    df_injuries : pandas DataFrame\n",
    "        DataFrame containing injury records with player_id and date columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Original df_main with added 'days_since_last_injury' column\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = df_main.copy()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'], errors='coerce')\n",
    "    injuries_df = df_injuries.copy()\n",
    "    injuries_df['date'] = pd.to_datetime(injuries_df['date'], errors='coerce')\n",
    "    \n",
    "    # Make sure player_id is consistent type\n",
    "    result_df['player_id'] = result_df['player_id'].astype(str)\n",
    "    injuries_df['player_id'] = injuries_df['player_id'].astype(str)\n",
    "    \n",
    "    # Create an empty column for days since last injury\n",
    "    result_df['days_since_last_injury'] = np.nan\n",
    "    \n",
    "    # Iterate through unique players\n",
    "    for player_id in result_df['player_id'].unique():\n",
    "        # Get all dates for this player\n",
    "        player_dates = result_df.loc[result_df['player_id'] == player_id, 'date']\n",
    "        if len(player_dates) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get all injury dates for this player\n",
    "        player_injury_dates = injuries_df.loc[injuries_df['player_id'] == player_id, 'date']\n",
    "        if len(player_injury_dates) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each date, find the most recent injury date before it\n",
    "        for idx, current_date in zip(player_dates.index, player_dates):\n",
    "            if pd.isna(current_date):\n",
    "                continue\n",
    "                \n",
    "            # Find most recent injury before current date\n",
    "            prior_injuries = player_injury_dates[player_injury_dates < current_date]\n",
    "            if len(prior_injuries) > 0:\n",
    "                most_recent_injury = max(prior_injuries)\n",
    "                days_since = (current_date - most_recent_injury).days\n",
    "                result_df.loc[idx, 'days_since_last_injury'] = days_since\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "########################################################\n",
    "# 5) Injuries in Last X Months (Optional)\n",
    "########################################################\n",
    "\n",
    "\n",
    "def compute_injuries_in_last_x_months(df_main, df_injuries, months=6):\n",
    "    \"\"\"\n",
    "    Count injuries in the last X months for each player in df_main.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_main : pandas DataFrame\n",
    "        Main DataFrame containing player_id and date columns\n",
    "    df_injuries : pandas DataFrame\n",
    "        DataFrame containing injury records with player_id and date columns\n",
    "    months : int, default=6\n",
    "        Number of months to look back for injuries\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Original df_main with added 'inj_count_last_Xm' column\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = df_main.copy()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'], errors='coerce')\n",
    "    injuries_df = df_injuries.copy()\n",
    "    injuries_df['date'] = pd.to_datetime(injuries_df['date'], errors='coerce')\n",
    "    \n",
    "    # Make sure player_id is consistent type\n",
    "    result_df['player_id'] = result_df['player_id'].astype(str)\n",
    "    injuries_df['player_id'] = injuries_df['player_id'].astype(str)\n",
    "    \n",
    "    # Create column name for injury count\n",
    "    count_col = f'inj_count_last_{months}m'\n",
    "    \n",
    "    # Initialize injury count column\n",
    "    result_df[count_col] = 0\n",
    "    \n",
    "    # Iterate through each row in the main dataframe\n",
    "    for idx, row in result_df.iterrows():\n",
    "        player_id = row['player_id']\n",
    "        current_date = row['date']\n",
    "        \n",
    "        if pd.isna(current_date):\n",
    "            result_df.loc[idx, count_col] = np.nan\n",
    "            continue\n",
    "            \n",
    "        # Define the time window\n",
    "        start_date = current_date - pd.DateOffset(months=months)\n",
    "        \n",
    "        # Count injuries in the window\n",
    "        count = len(injuries_df[(injuries_df['player_id'] == player_id) & \n",
    "                              (injuries_df['date'] >= start_date) & \n",
    "                              (injuries_df['date'] < current_date)])\n",
    "        \n",
    "        result_df.loc[idx, count_col] = count\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "########################################################\n",
    "# 6) Build All New Features from raw stats\n",
    "########################################################\n",
    "\n",
    "all_features_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    # 1) player aggregator\n",
    "    p_feats = aggregate_player_stats(row[\"raw_player_team_fixture_stats\"])\n",
    "    \n",
    "    # 2) team aggregator\n",
    "    t_feats = aggregate_team_stats(row[\"raw_team_fixture_agg_stats\"])\n",
    "    \n",
    "    # 3) opponent aggregator\n",
    "    o_feats = aggregate_opponent_stats(row[\"raw_opp_fixture_agg_stats\"])\n",
    "    \n",
    "    # Combine them\n",
    "    combined = {}\n",
    "    combined.update(p_feats)\n",
    "    combined.update(t_feats)\n",
    "    combined.update(o_feats)\n",
    "    \n",
    "    all_features_list.append(combined)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "features_df = pd.DataFrame(all_features_list)\n",
    "\n",
    "# Join these columns onto df\n",
    "df = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "########################################################\n",
    "# 7) Optionally, Add \"days_since_last_injury\" or \"injuries in last X months\"\n",
    "########################################################\n",
    "\n",
    "# If you have a separate injuries DataFrame called df_injuries with columns:\n",
    "#   [player_id, date, reason, etc. for each injury event]\n",
    "# Then do:\n",
    "\n",
    "df = compute_days_since_last_injury(df, df_injuries)\n",
    "df = compute_injuries_in_last_x_months(df, df_injuries, months=6)\n",
    "\n",
    "########################################################\n",
    "# 8) Additional Interactions (Team vs Opponent)\n",
    "########################################################\n",
    "# 1) Simple Differences\n",
    "df[\"team_vs_opp_shots_diff_5\"] = df[\"team_total_shots_5\"] - df[\"opp_total_shots_5\"]\n",
    "df[\"team_vs_opp_sog_diff_5\"] = df[\"team_shots_on_goal_5\"] - df[\"opp_shots_on_goal_5\"]\n",
    "df[\"team_vs_opp_fouls_diff_5\"] = df[\"team_fouls_5\"] - df[\"opp_fouls_5\"]\n",
    "df[\"team_vs_opp_corners_diff_5\"] = df[\"team_corners_5\"] - df[\"opp_corners_5\"]\n",
    "df[\"team_vs_opp_offsides_diff_5\"] = df[\"team_offsides_5\"] - df[\"opp_offsides_5\"]\n",
    "\n",
    "# Ball possession is an average in percent, so a difference here can show which side \n",
    "# tends to have a higher possession rating in recent matches.\n",
    "df[\"team_vs_opp_poss_diff_5\"] = df[\"team_ball_poss_avg_5\"] - df[\"opp_ball_poss_avg_5\"]\n",
    "\n",
    "# Pass accuracy ratio might also be an average or ratio. \n",
    "# But if you prefer to do difference of pass_acc_ratio_5:\n",
    "df[\"team_vs_opp_pass_acc_diff_5\"] = df[\"team_pass_acc_ratio_5\"] - df[\"opp_pass_acc_ratio_5\"]\n",
    "\n",
    "# 2) Ratios\n",
    "# For ratios, we use np.where() to avoid dividing by zero or NaN.\n",
    "\n",
    "df[\"team_vs_opp_shots_ratio_5\"] = np.where(\n",
    "    (df[\"opp_total_shots_5\"].isna()) | (df[\"opp_total_shots_5\"] == 0),\n",
    "    np.nan,\n",
    "    df[\"team_total_shots_5\"] / df[\"opp_total_shots_5\"]\n",
    ")\n",
    "\n",
    "df[\"team_vs_opp_sog_ratio_5\"] = np.where(\n",
    "    (df[\"opp_shots_on_goal_5\"].isna()) | (df[\"opp_shots_on_goal_5\"] == 0),\n",
    "    np.nan,\n",
    "    df[\"team_shots_on_goal_5\"] / df[\"opp_shots_on_goal_5\"]\n",
    ")\n",
    "\n",
    "df[\"team_vs_opp_fouls_ratio_5\"] = np.where(\n",
    "    (df[\"opp_fouls_5\"].isna()) | (df[\"opp_fouls_5\"] == 0),\n",
    "    np.nan,\n",
    "    df[\"team_fouls_5\"] / df[\"opp_fouls_5\"]\n",
    ")\n",
    "\n",
    "df[\"team_vs_opp_corners_ratio_5\"] = np.where(\n",
    "    (df[\"opp_corners_5\"].isna()) | (df[\"opp_corners_5\"] == 0),\n",
    "    np.nan,\n",
    "    df[\"team_corners_5\"] / df[\"opp_corners_5\"]\n",
    ")\n",
    "\n",
    "df[\"team_vs_opp_offsides_ratio_5\"] = np.where(\n",
    "    (df[\"opp_offsides_5\"].isna()) | (df[\"opp_offsides_5\"] == 0),\n",
    "    np.nan,\n",
    "    df[\"team_offsides_5\"] / df[\"opp_offsides_5\"]\n",
    ")\n",
    "\n",
    "# If you want a ratio for ball possession, e.g. how big is \n",
    "# your team's possession share relative to opponent's\n",
    "df[\"team_vs_opp_poss_ratio_5\"] = np.where(\n",
    "    (df[\"opp_ball_poss_avg_5\"].isna()) | (df[\"opp_ball_poss_avg_5\"] == 0),\n",
    "    np.nan,\n",
    "    df[\"team_ball_poss_avg_5\"] / df[\"opp_ball_poss_avg_5\"]\n",
    ")\n",
    "\n",
    "# Pass accuracy ratio (comparing your team's pass_acc_ratio_5 to opponent's)\n",
    "df[\"team_vs_opp_pass_acc_ratio_5\"] = np.where(\n",
    "    (df[\"opp_pass_acc_ratio_5\"].isna()) | (df[\"opp_pass_acc_ratio_5\"] == 0),\n",
    "    np.nan,\n",
    "    df[\"team_pass_acc_ratio_5\"] / df[\"opp_pass_acc_ratio_5\"]\n",
    ")\n",
    "\n",
    "########################################################\n",
    "# 9) Drop/Exclude ID Columns, Keep Features + Target\n",
    "########################################################\n",
    "# We'll assume your target is \"injuried\"\n",
    "# We'll exclude typical ID columns, plus raw stats columns if you want.\n",
    "\n",
    "exclude_cols = {\n",
    "    \"player_id\", \"fixture_id\", \"team_id\", \"league_id\", \"opponent_team_id\",\n",
    "    \"prev_inj_team_fixture_1\", \"prev_inj_team_fixture_2\", \"prev_inj_team_fixture_3\",\n",
    "    \"prev_inj_team_fixture_4\", \"prev_inj_team_fixture_5\",\n",
    "    \"prev_opp_team_fixture_1\", \"prev_opp_team_fixture_2\", \"prev_opp_team_fixture_3\",\n",
    "    \"prev_opp_team_fixture_4\", \"prev_opp_team_fixture_5\",\n",
    "    \"raw_player_team_fixture_stats\", \"raw_team_fixture_agg_stats\", \"raw_opp_fixture_agg_stats\",\n",
    "    \"date\",\n",
    "    'prev_player_id', 'prev_player_name', 'prev_player_firstname', 'prev_player_lastname', 'prev_player_birth_date', \n",
    "    'prev_player_birth_place', 'prev_player_birth_country', 'prev_player_nationality',\"prev_player_injured\",'prev_player_photo', \n",
    "    'prev_team_id', 'prev_team_name', 'prev_team_logo', 'prev_league_id', 'prev_league_name', 'prev_league_country', 'prev_league_logo', \n",
    "    'prev_league_flag', 'prev_league_season',\n",
    "    # \"inj_count_last_6m\",  # If you want to exclude or keep it, your choice\n",
    "    # If you want to exclude \"date\", you can add it here\n",
    "}\n",
    "\n",
    "all_cols = df.columns.tolist()\n",
    "final_cols = []\n",
    "for c in all_cols:\n",
    "    if c in exclude_cols:\n",
    "        continue\n",
    "    # Keep target \"injuried\" plus any that start with \"player_\", \"team_\", \"opp_\",\n",
    "    # or end with \"_diff_5\" or \"_ratio_5\", or \"days_since_last_injury\"\n",
    "    if (c == \"injuried\"\n",
    "        or c.startswith(\"player_\")\n",
    "        or c.startswith(\"team_\")\n",
    "        or c.startswith(\"opp_\")\n",
    "        or c.startswith(\"prev_\")\n",
    "        or c.startswith(\"inj_\")\n",
    "        or c.endswith(\"_diff_5\")\n",
    "        or c.endswith(\"_ratio_5\")\n",
    "        or c == \"days_since_last_injury\"):\n",
    "        final_cols.append(c)\n",
    "\n",
    "final_features_df = df[final_cols].copy()\n",
    "\n",
    "print(\"Final feature columns for modeling:\")\n",
    "print(final_features_df.columns.tolist())\n",
    "\n",
    "print(\"Sample of final features:\")\n",
    "print(final_features_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:58:41.533252Z",
     "iopub.status.busy": "2025-04-08T19:58:41.532777Z",
     "iopub.status.idle": "2025-04-08T19:58:41.547637Z",
     "shell.execute_reply": "2025-04-08T19:58:41.546243Z",
     "shell.execute_reply.started": "2025-04-08T19:58:41.533216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values().tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:12:04.028166Z",
     "iopub.status.busy": "2025-04-08T17:12:04.027759Z",
     "iopub.status.idle": "2025-04-08T17:12:04.166189Z",
     "shell.execute_reply": "2025-04-08T17:12:04.164381Z",
     "shell.execute_reply.started": "2025-04-08T17:12:04.028136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"final_df2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7086591,
     "sourceId": 11328764,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
